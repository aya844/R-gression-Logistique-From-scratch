{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TP : Régression Logistique (from scratch) — Méthode de Newton / IRLS\n",
    "\n",
    "Dans ce notebook, nous implémentons la régression logistique utilisant la méthode de Newton (aussi appelée IRLS : Iteratively Reweighted Least Squares).\n",
    "\n",
    "Cette méthode repose sur :\n",
    "- la log-vraisemblance,\n",
    "- son gradient,\n",
    "- sa matrice Hessienne,\n",
    "- et la mise à jour de Newton-Raphson.\n",
    "\n",
    "Elle converge beaucoup plus rapidement que la descente de gradient.\n"
   ],
   "id": "2ea0ce93ec670d14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T21:38:27.233297Z",
     "start_time": "2025-12-06T21:38:27.224463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "feature = 'Glucose'\n",
    "X = df[[feature]].values.astype(float)\n",
    "y = df['Outcome'].values.astype(int)\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X).reshape(-1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.5, random_state=0)\n"
   ],
   "id": "b1ec40ccf9661623",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modèle et rappels mathématiques\n",
    "\n",
    "Nous gardons le même modèle :\n",
    "\n",
    "$$\n",
    "p_i = \\sigma(\\beta_0 + \\beta_1 x_i).\n",
    "$$\n",
    "\n",
    "La log-vraisemblance :\n",
    "\n",
    "$$\n",
    "\\ell(\\beta)\n",
    "= \\sum_{i=1}^n \\left[\n",
    "y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "### Gradient\n",
    "Déjà dérivé :\n",
    "\n",
    "$$\n",
    "g(\\beta)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\sum (y_i - p_i) \\\\\n",
    "\\sum x_i (y_i - p_i)\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "### Hessienne\n",
    "\n",
    "On dérive à nouveau :\n",
    "\n",
    "$$\n",
    "p_i(1-p_i) = w_i \\quad\\text{(poids IRLS)}.\n",
    "$$\n",
    "\n",
    "La Hessienne de $\\ell(\\beta)$ vaut :\n",
    "\n",
    "$$\n",
    "H(\\beta)\n",
    "=\n",
    "-\\sum_{i=1}^n\n",
    "w_i\n",
    "\\begin{pmatrix}\n",
    "1 & x_i \\\\\n",
    "x_i & x_i^2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "La mise à jour de Newton pour **maximiser** $\\ell$ est :\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - H(\\beta^{(t)})^{-1} g(\\beta^{(t)}).\n",
    "$$\n",
    "\n",
    "C’est l’algorithme IRLS classique.\n"
   ],
   "id": "41f26c143b95f84b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interprétation de la méthode IRLS (Iteratively Reweighted Least Squares)\n",
    "\n",
    "La méthode de Newton appliquée à la régression logistique est aussi appelée **IRLS**, pour *Iteratively Reweighted Least Squares* (Moindres Carrés Pondérés Itératifs).\n",
    "\n",
    "Cela signifie que, à chaque itération, l’algorithme résout un problème de moindres carrés où chaque observation est pondérée par un poids :\n",
    "\n",
    "$$\n",
    "w_i = p_i(1 - p_i).\n",
    "$$\n",
    "\n",
    "Ces poids dépendent de la probabilité prédite au point courant, d’où le terme *iteratively reweighted*.\n",
    "\n",
    "Concrètement, l’algorithme IRLS consiste à résoudre à chaque itération un système linéaire de la forme :\n",
    "\n",
    "$$\n",
    "(X^\\top W X)\\,\\Delta = X^\\top (y - p),\n",
    "$$\n",
    "\n",
    "ce qui correspond exactement à la mise à jour de Newton pour maximiser la log-vraisemblance :\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - H(\\beta^{(t)})^{-1}\\nabla\\ell(\\beta^{(t)}),\n",
    "$$\n",
    "\n",
    "où la Hessienne est donnée par :\n",
    "\n",
    "$$\n",
    "H(\\beta)\n",
    "= -\\sum_{i=1}^n w_i\n",
    "\\begin{pmatrix}\n",
    "1 & x_i \\\\\n",
    "x_i & x_i^2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "IRLS est très efficace car il convergé généralement en très peu d’itérations, contrairement à la descente de gradient.\n"
   ],
   "id": "1675bc61319cff90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implémentation des fonctions nécessaires\n",
    "\n",
    "Nous définissons :\n",
    "- la sigmoïde,\n",
    "- la log-vraisemblance,\n",
    "- le gradient,\n",
    "- la Hessienne,\n",
    "- et l'algorithme IRLS."
   ],
   "id": "7f677d03aa47d18d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T21:58:11.746723Z",
     "start_time": "2025-12-06T21:58:11.735771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def log_vraisemblance(beta, X, y):\n",
    "    # beta: array([b0, b1])\n",
    "    z = beta[0] + beta[1]*X\n",
    "    p = sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    return np.sum(y*np.log(p+eps) + (1-y)*np.log(1-p+eps))\n",
    "\n",
    "def hessienne(beta, X):\n",
    "    z = beta[0] + beta[1]*X\n",
    "    p = sigmoid(z)\n",
    "    w = p * (1-p)\n",
    "    H00 = -np.sum(w)\n",
    "    H01 = -np.sum(X * w)\n",
    "    H11 = -np.sum((X**2) * w)\n",
    "    H = np.array([[H00, H01],\n",
    "                  [H01, H11]])\n",
    "    return H\n",
    "\n",
    "def newton_irls(X, y, max_iter=20, tol=1e-8, damping=0.0, verbose=False):\n",
    "    beta = np.zeros(2)\n",
    "    for k in range(max_iter):\n",
    "        z = beta[0] + beta[1]*X\n",
    "        p = sigmoid(z)\n",
    "        g0 = np.sum(y - p)\n",
    "        g1 = np.sum(X * (y - p))\n",
    "        g = np.array([g0, g1])\n",
    "        H = hessienne(beta, X)\n",
    "        H_reg = H.copy()\n",
    "        if damping>0:\n",
    "            H_reg[0,0] -= damping\n",
    "            H_reg[1,1] -= damping\n",
    "        try:\n",
    "            delta = np.linalg.solve(H_reg, g)  # résourdre H delta = g\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Hessian singular, add damping\")\n",
    "            H_reg = H_reg - np.eye(2)*1e-6\n",
    "            delta = np.linalg.solve(H_reg, g)\n",
    "        beta_new = beta - delta\n",
    "        if verbose:\n",
    "            print(f\"iter {k}: beta={beta}, ll={log_vraisemblance(beta,X,y)}\")\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "        beta = beta_new\n",
    "    return beta\n",
    "\n",
    "#Entrainement du modéle avec Newton / IRLS\n",
    "beta_newton = newton_irls(X_train, y_train, max_iter=50, damping=1e-6, verbose=True)\n",
    "beta_newton\n"
   ],
   "id": "91f215a9dc2a67b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: beta=[0. 0.], ll=-266.168517334251\n",
      "iter 1: beta=[-0.59560932  0.85801941], ll=-210.1208406710374\n",
      "iter 2: beta=[-0.7341563   1.11428207], ll=-207.55866553749044\n",
      "iter 3: beta=[-0.75085317  1.14640865], ll=-207.52853928549837\n",
      "iter 4: beta=[-0.75108412  1.14686894], ll=-207.5285334084336\n",
      "iter 5: beta=[-0.75108417  1.14686904], ll=-207.52853340843336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.75108417,  1.14686904])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparaison avec une implémentation standard\n",
    "\n",
    "`sklearn.linear_model.LogisticRegression` utilise en interne une variante de Newton (liblinear ou lbfgs).\n",
    "\n",
    "Comparer :\n",
    "- coefficients,\n",
    "- accuracy,\n",
    "- AUC,\n",
    "\n",
    "permet de vérifier la justesse de notre implémentation IRLS from scratch.\n"
   ],
   "id": "36257392a4f1c576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T21:47:00.313211Z",
     "start_time": "2025-12-06T21:47:00.299694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluation\n",
    "def predict(beta, X, thr=0.5):\n",
    "    return (sigmoid(beta[0] + beta[1]*X) >= thr).astype(int)\n",
    "\n",
    "print(\"Newton beta:\", beta_newton)\n",
    "y_pred_test = predict(beta_newton, X_test)\n",
    "print(\"Test accuracy (Newton):\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Test AUC (Newton):\", roc_auc_score(y_test, sigmoid(beta_newton[0] + beta_newton[1]*X_test)))\n",
    "\n",
    "# comparaison sklearn\n",
    "clf = LogisticRegression(solver='lbfgs').fit(X_train.reshape(-1,1), y_train)\n",
    "print(\"Sklearn beta:\", np.array([clf.intercept_[0], clf.coef_[0,0]]))\n"
   ],
   "id": "3e9410404e9f1906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton beta: [-0.75108417  1.14686904]\n",
      "Test accuracy (Newton): 0.7421875\n",
      "Test AUC (Newton): 0.7981021633527441\n",
      "Sklearn beta: [-0.74658524  1.12416839]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Variance asymptotique des estimateurs\n",
    "On peut estimer la matrice de covariance asymptotique des estimateurs par l'inverse de l'information de Fisher.\n",
    "\n",
    "En estimation par Maximum de Vraisemblance :\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{\\beta}) \\approx I(\\hat{\\beta})^{-1},\n",
    "$$\n",
    "\n",
    "où l'information observée vaut :\n",
    "\n",
    "$$\n",
    "I(\\beta)\n",
    "=\n",
    "\\sum_{i=1} w_i\n",
    "\\begin{pmatrix}\n",
    "1 & x_i \\\\\n",
    "x_i & x_i^2\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "w_i = p_i(1-p_i).\n",
    "$$\n",
    "\n",
    "Cela permet d’obtenir :\n",
    "- erreurs standards,\n",
    "- intervalles de confiance,\n",
    "- tests de Wald.\n"
   ],
   "id": "4b7e687a2cc24fa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T22:06:01.352803Z",
     "start_time": "2025-12-06T22:06:01.343694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# estimation de la variance asymptotique (information observée)\n",
    "beta_hat = beta_newton\n",
    "z = beta_hat[0] + beta_hat[1]*X_train\n",
    "p = sigmoid(z)\n",
    "W = p*(1-p)\n",
    "I_obs = np.array([[np.sum(W), np.sum(X_train*W)],\n",
    "                  [np.sum(X_train*W), np.sum(X_train**2 * W)]])\n",
    "cov_beta = np.linalg.inv(I_obs)\n",
    "se = np.sqrt(np.diag(cov_beta))\n",
    "print(\"beta_hat:\", beta_hat)\n",
    "print(\"SE:\", se)\n",
    "# 95% CI\n",
    "from scipy.stats import norm\n",
    "z95 = norm.ppf(0.975)\n",
    "ci = np.vstack([beta_hat - z95*se, beta_hat + z95*se]).T\n",
    "print(\"95% CI:\", ci)\n"
   ],
   "id": "7f3fed571a70fd1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat: [-0.75108417  1.14686904]\n",
      "SE: [0.12336582 0.14275697]\n",
      "95% CI: [[-0.99287673 -0.5092916 ]\n",
      " [ 0.86707053  1.42666755]]\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
